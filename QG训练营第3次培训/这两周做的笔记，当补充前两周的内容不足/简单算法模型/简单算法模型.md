<center>简单算法模型</center>

就k-means算法我之前没有摸过，其他的三个都自己搓了一遍阉割版的算法。所以可能就k-means算法我会事无巨细的记一下，其他的就比较简略了，就记录一些比较生疏的用法。

# KNN

这个knn显然也是阉割版的，但是一定会比第一次的阉割版要，好一点的叭

### 原理

近朱者赤，近墨者黑

### 随手记

argsort()就是用来排序的，对一个数组从小到大进行排序，但是它返回的是排序后结果的索引

`nearest = np.argsort(distances)`

截取了前k个元素之后，它会返回一个列表，那么这个列表里面某一类的元素可能是最多的，我们需要那个类作为knn的算法模型。

```python
topK_y = [y_train[i] for i in nearest[:k]]
```

那么怎么取呢，就最好用Counter，不过Counter返回的东西大概长这个样子{A:proportion of A,B:proportion of B,…}这样的键值对构成的。相当于A 有proportion of A这么多票，B同理。

然后用most_common(1)就取得票数最多的元素，但是most_common()返回的是个列表，以元组为单位，因为要考虑到会有取两位三位更多位的情况嘛，所以需要列表进行封装。然后再用[0]取得第一个元组，再用一次[0]取得元组中的第一个元素，就是票数最多的类。

```python
from collections import Counter

votes = Counter(topK_y)

return votes.most_common(1)[0][0]
```

### 来点优化

哎，但是我写到一半觉得不能这样，要来点优化。我想要对距离分配权重，也就是说这次knn算法我就统一用加权投票法来处理距离好了。

一方面要给自己加一点点点难度，另一方面这个样本的比例差距实在是有点大，分配了权重之后可能准确度会更令人满意一些。也就是上面的Counter我就不用了。

此外，好像变量数越多时，欧式距离的区分能力就会越差，那到时候，可能还要改改。

#### 距离加权

感谢这位博主的[博客](https://blog.csdn.net/weixin_41770169/article/details/81560946)，就是没有把代码写出来~~白嫖失败~~

权重，我使用的是高斯函数。这个在距离为0的时候权重为1，随着距离增大，权重减少，但不会变为0。因为在k中，肯定有更多的样本的label是没有违规的，我需要降低它们的权重。

```python
def gaussian(dist, a=1, b=0, c=0.3):
	return a * math.e ** (-(dist - b) ** 2 / (2 * c ** 2))
```

我需要定义一个”概率“的东西。

```
1.加权kNN首先获得经过排序的距离值，再取距离最近的k个元素。

2.在处理离散型数据时，将这k个数据用权重区别对待，预测结果与第n个数据的label相同的概率：

3.将各个类预测的权重值相加，哪个类最大，就属于哪个类。

4.f(x) = Wi属于类x / Wi总和      i=1,2,...,k
```

#### 寻找最好的K

我们怎么选择超参数啊，那只能实验了。

```python
best_score = 0.0
besk_k = -1
for i in range(1,11):
    knn_clf = KNNClassifier(k=i)
    knn_clf.fit(X_train, y_train)
    score = knn_clf.score(X_test, y_test)
    if score > best_score：
    	best_k = k
        best_score = score
print("best_k = ",best_k)
print("best_score ="est_score)
```



# Linear_Regression

啊呐，也意思一下就好。不过SimpleLinearRegression的原理没什么好说的，但是它的一个一个评估指标都很有意思，我们能够看到说，一开始是一些简单的想法去衡量模型，但是指标有自己的缺陷，然后我们再想办法改进它。这是一种很生动的体验。

### SimpleLinearRegression

莫得

~~又怂又耿直地叉腰~~

### LinearRegression

这里来一手多元和正规方程法。虽然正规方程法的数学原理我也不是门清儿，但是大概对数学原理有个感觉，吹爆MIT的线性代数，这是妈妈问我为什么跪着听课系列。我必须把它编进去。

啊嘞啊嘞，正规方程法的原理我只是有个感性认识来着的，但我发现我的梯度下降法的认识还没有寒假清楚了。看看叭，我再看看要不要记，因为现在对梯度的认识上升一个台阶了。再看原来的数学公式脑子需要更新一下。

注意为了匹配$\theta$向量和$X$向量的个数，我们的$X_0\equiv1$，具体的在代码中会有这么一手操作，注意区分。

```python
X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
```



### 梯度下降法

emmm，关键就是求梯度，然后对向量中的每一个项减一减，开倒车嘀嘀嘀。哎应该不用记了。我也不想再搞什么图片。

~~哎草~~还是记一下叭,虽然原理差不多了，但是脑子中的一些细节还需要雕琢一下，就啰啰嗦嗦记一下。

因为我们的模型是这个样子的：

$\hat{y}^{(i)}=\theta_0+\theta_1X_1^{(i)}+\theta_2X_2^{(i)}+...+\theta_nX_n^{(i)}$

所以我们的目标应该是使$\sum\limits_{i=1}^m({y}^{(i)}-\theta_0-\theta_1X_1^{(i)}-\theta_2X_2^{(i)}-...-\theta_nX_n^{(i)})^2$尽可能的小。

$\bigtriangledown J(\theta)=\begin{equation}\left( \begin{array}{c}\partial J/\partial \theta_0\\\partial J/\partial\theta_1\\...\\J/\partial\theta_n\end{array}\right)\end{equation}$$= \begin{equation}\left( \begin{array}{c}\sum\limits_{i=1}^m2(y^{(i)}-X^{(i)}_b)*(-1)\\\sum\limits_{i=1}^m2(y^{(i)}-X^{(i)}_b)*(-X_1^{(i)})\\...\\\sum\limits_{i=1}^m2(y^{(i)}-X^{(i)}_b)*(-X_n^{(i)})\end{array}\right)\end{equation}$

这么复杂怎么顶啊，那肯定要进行向量化运算嘛把梯度拆开，我实在不想再打复杂的公式了呜呜呜最后化简完就是${\dfrac{2}{m}*X^T_b*(X_b\theta-y)}$

```python
 X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)
```

# Logistic_Regression

听上去像是一个回归算法，为什么是解决分类问题的？这是因为它把样本的特征和样本发生的概率联系起来了。比如病性肿瘤的概率预测出来恶性的可能性是0.7的话，那我们可能就把它的肿瘤分到恶性的类中去了。在推广开之前，我们只能用它解决二分类的问题。

所以底子是由回归打的，所以线性回归，多项式回归都可以上。但这样不能保证$\hat{y}=\theta^T·x$输出的数在0\~1之间(那样就不是概率了)。为了保证这个对接，我们再用一个函数$\sigma(t)=\dfrac{1}{1+e^{-t}}$导一下，我们就把输出值压缩到0~1之间了。我们规定，这个输出值就是它的概率。

总结一下，就是$\hat{p}=\sigma(\theta^T·x)=\dfrac{1}{1+e^{-\theta^T·x}}$。当$\hat{p}$大于等于0.5的时候，我们就预测其为1类，否则预测其为0类。

**代价函数：**

这个东西乍一想可能比方差什么的难规定一些，但是逻辑是差不多的。我们抓住$\hat{p}$和$y$（也就是具体的类，只能是0或者1嘛）之间的差距，如果如果这个差距越小，比如一个东西是1类，你预测它是1类的可能性是0.9。那么就越准。我们用这个样子的形式去规定我们的代价函数。

$cost=-ylog(\hat{p})-(1-y)log(1-\hat{p})$

以什么为底其实不太重要，关键是图像的走势。

上面的表达式都是针对一个样本的，$J(\theta)$只需要将所有的cost累加然后除以总量m即可得到平均的代价函数。

$J(\theta)=-\dfrac{\sum\limits_{i=1}^my^{(i)}log(\sigma(X_b^{(i)}\theta))+(1-y^{(i)})log(1-\sigma(X_b^{(i)}\theta)}{m}$

这个函数没有解析式，也就是说我们没有公式可以直接套进去算出函数的最小值，也就是说我们只能**用梯度下降法来求解**了。也正是因为这个原因，我们在众多可以表达预测准确度的函数表达式中选择了这个，因为它是一个下凸函数，也就是说只有全局最优解。很合适用梯度下降法。它的导函数$\dfrac{J(\theta)}{\theta_j}=\dfrac{\sum\limits_{i=1}^my^{(i)}(\sigma(X_b^{(i)}\theta)-y^{(i)})X_j^{(i)}}{m}$推导起来挺繁琐的，就直接用好了。

所以我们就很方便的有$\bigtriangledown J(\theta)=\begin{equation}\left( \begin{array}{c}\partial J/\partial \theta_0\\\partial J/\partial\theta_1\\...\\J/\partial\theta_n\end{array}\right)\end{equation}$因为在编程的时候我们希望计算操作是向量化的，所以再给出它的向量化公式$\bigtriangledown J(\theta)=\dfrac{X^T_b·(\sigma(X_b\theta)-y)}{m}$



# K_means

